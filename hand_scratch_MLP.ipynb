{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "6250ff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "61a5c717",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.output = np.zeros(2)\n",
    "        \n",
    "        self.widths = []\n",
    "        self.activation_funcs = []\n",
    "        self.weights = []\n",
    "        self.bias = []\n",
    "        self.layers = []\n",
    "        \n",
    "        self.momentum_weights = []\n",
    "        self.momentum_bias = []\n",
    "        \n",
    "    def Dense(self, n_neurons, activation_func):\n",
    "        #input size for this layer, output_len = n_neurons\n",
    "        prev_n_neurons = self.widths[-1] if self.widths else self.X.shape[1]\n",
    "        # -- adjustment:() or not? normal or rand? -- \n",
    "        weights = np.random.rand(n_neurons, prev_n_neurons) * np.sqrt(2 / (n_neurons + prev_n_neurons))\n",
    "        #-- bias Initialize to 0s -- \n",
    "        bias = np.zeros((n_neurons,1))\n",
    "        layer = np.random.rand(n_neurons)\n",
    "        self.widths.append(n_neurons)\n",
    "        self.activation_funcs.append(activation_func)\n",
    "        self.weights.append(weights)\n",
    "        self.bias.append(bias)\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def OutputLayer(self):\n",
    "        prev_n_neurons = self.widths[-1]\n",
    "        weights = np.random.rand(self.output.shape[0], prev_n_neurons) * np.sqrt(2 / (self.output.shape[0] + prev_n_neurons))\n",
    "        bias = np.zeros((2,1))\n",
    "        \n",
    "        self.weights.append(weights)\n",
    "        self.bias.append(bias)\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def Momentum(self):\n",
    "        for i in range(len(self.widths)):\n",
    "            weights = np.zeros(self.weights[i].shape)\n",
    "            bias = np.zeros(self.bias[i].shape)\n",
    "            \n",
    "            self.momentum_weights.append(weights)\n",
    "            self.momentum_bias.append(bias)\n",
    "            \n",
    "        output_momentum_weights = np.zeros(self.weights[-1].shape)\n",
    "        output_momentum_bias = np.zeros(self.bias[-1].shape)\n",
    "        self.momentum_weights.append(output_momentum_weights)\n",
    "        self.momentum_bias.append(output_momentum_bias)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def Forward(self, X):\n",
    "        for i in range(len(self.widths)):\n",
    "            width = self.widths[i]\n",
    "            weights = np.asarray(self.weights[i])\n",
    "            bias = np.asarray(self.bias[i])\n",
    "            activation_func = self.activation_funcs[i]\n",
    "            if activation_func == 'relu':\n",
    "                output = self.ReLU(np.matmul(weights, X.T) + bias)\n",
    "            elif activation_func == 'sigmoid':\n",
    "                output = self.Sigmoid(np.matmul(weights, X.T) + bias)\n",
    "            output = output.T\n",
    "            self.layers[i] = output\n",
    "            X = output\n",
    "        output_weights = np.asarray(self.weights[-1])\n",
    "        output_bias = np.asarray(self.bias[-1])\n",
    "        self.output = self.Softmax(np.matmul(output_weights, X.T) + output_bias)\n",
    "        \n",
    "        return self, self.output\n",
    "    \n",
    "    def Back(self, output, real_labels, learning_rate, beta, X):\n",
    "        \n",
    "        error = np.asarray(output - real_labels)\n",
    "        \n",
    "        output_delta_weights = np.matmul(error.T, self.layers[-1]) * (1./output.shape[0])\n",
    "        output_delta_bias = np.sum(error, axis=0, keepdims=True).T * (1./output.shape[0])\n",
    "        \n",
    "        self.momentum_weights[-1] = beta * np.asarray(self.momentum_weights[-1]) + (1. - beta) * output_delta_weights\n",
    "        self.momentum_bias[-1] = beta * np.asarray(self.momentum_bias[-1])  + (1. - beta) * output_delta_bias\n",
    "        \n",
    "        self.weights[-1] = np.asarray(self.weights[-1]) - learning_rate * np.asarray(self.momentum_weights[-1])\n",
    "        self.bias[-1] = np.asarray(self.bias[-1]) - learning_rate * np.asarray(self.momentum_bias[-1])\n",
    "        \n",
    "        error = error.T\n",
    "        delta_layer = error\n",
    "        for i in range(len(self.widths)-1, -1, -1):\n",
    "            layer = np.asarray(self.layers[i].T)\n",
    "            delta_layer = np.matmul(self.weights[i+1].T, delta_layer)* layer * (1 - layer)\n",
    "            \n",
    "            if i == 0: \n",
    "                delta_weights = np.matmul(delta_layer, X) * (1./output.shape[0])\n",
    "            else:\n",
    "                delta_weights = np.matmul(delta_layer, self.layers[i-1]) * (1./output.shape[0])\n",
    "                \n",
    "            delta_bias = np.sum(delta_layer, axis=1, keepdims=True) * (1./output.shape[0])\n",
    "            \n",
    "            self.weights[i] = np.asarray(self.weights[i]) - learning_rate * self.momentum_weights[i]\n",
    "            self.bias[i] = np.asarray(self.bias[i]) - learning_rate * self.momentum_bias[i]\n",
    "            \n",
    "            self.momentum_weights[i] = beta * np.asarray(self.momentum_weights[i]) + (1. - beta) * delta_weights\n",
    "            self.momentum_bias[i] = beta * np.asarray(self.momentum_bias[i])  + (1. - beta) * delta_bias\n",
    "        \n",
    "        return self \n",
    "    \n",
    "    def Predict(self, X, y):\n",
    "        preds = []\n",
    "        acc_cnt = 0\n",
    "        for i in range(X.shape[0]):\n",
    "            _, pred = self.Forward(X[i].reshape(1, X.shape[1]))\n",
    "            preds.append(np.where(pred == np.amax(pred), 1., 0.).T[0])\n",
    "        for i in range(len(preds)):\n",
    "            if np.array_equal(preds[i], y[i]):\n",
    "                acc_cnt += 1\n",
    "        acc = acc_cnt / y.shape[0]\n",
    "        return preds, acc\n",
    "    \n",
    "    def Training(self, epochs, batch_size, learning_rate, beta):\n",
    "        loss = []\n",
    "        acc = []\n",
    "        batches = self.X.shape[0] // batch_size\n",
    "        #print(\"nb of batches :\", batches)\n",
    "        \n",
    "        #X_train, X_val, y_train, Y_val = train_test_split(self.X, self.y, test_size=0.20)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            X_train, y_train = self.shuffle(self.X, self.y)\n",
    "            avg_cost = 0\n",
    "            cnt = 0\n",
    "            for batch in range(batches):\n",
    "                cnt += 1\n",
    "                start = batch * batch_size\n",
    "                end = min(start + batch_size, X_train.shape[0]-1)\n",
    "                if start < end:\n",
    "                    X_batch, y_batch = X_train[start:end], y_train[start:end]\n",
    "                    _, output = self.Forward(np.asarray(X_batch))\n",
    "                    output = output.T\n",
    "                    train_cost = self.CrossEntropyLoss(output, y_batch)\n",
    "                    avg_cost += train_cost\n",
    "                    self.Back(output, y_batch, learning_rate, beta, np.asarray(X_batch))\n",
    "                    \n",
    "             # Validation accuracy:\n",
    "            #pred_val, val_acc = self.Predict(X_val, Y_val)\n",
    "            \n",
    "            # Train accuracy:\n",
    "            #pred, train_acc = self.Predict(self.X, self.y)\n",
    "        \n",
    "            #print(\" * Epoch {}: Average cost = {}, Train_acc = {}, Val_acc = {}  * \".format(epoch+1, avg_cost/batches, train_acc, val_acc))\n",
    "            #print(\"*******************************************************\")\n",
    "            \n",
    "            # Store average training loss  and training accuracy value\n",
    "            #epoch_loss = avg_cost/batches\n",
    "            #loss.append(epoch_loss)\n",
    "            #acc.append(train_acc)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    #activation functions: ReLU, Sigmoid\n",
    "    def ReLU(self, z):\n",
    "        return z * (z > 0)\n",
    "\n",
    "    def Sigmoid(self, z):\n",
    "        return 1. / (1 + np.exp(-z))\n",
    "\n",
    "    #for output layer\n",
    "    def Softmax(self, z):\n",
    "        return np.exp(z) / np.sum(np.exp(z), axis = 0) \n",
    "    \n",
    "    #shuffle function\n",
    "    def shuffle(self, a, b):   \n",
    "        p = np.random.permutation(len(a))\n",
    "        return a[p], b[p]\n",
    "    \n",
    "    def CrossEntropyLoss(self, preds, true_labels):\n",
    "        return  - (1 / preds.shape[0]) * np.sum(np.multiply(true_labels, np.log(preds)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "8b24a464",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = ['xor', 'circle', 'spiral', 'gaussian']\n",
    "labels = ['_train_data.csv', '_train_label.csv', '_test_data.csv', '_test_label.csv',]\n",
    "location = 'public/'\n",
    "#dataset already shuffled no need to bother this\n",
    "X_train = np.loadtxt(location + cases[3] + labels[0], delimiter = ',')\n",
    "Y_train = np.loadtxt(location + cases[3] + labels[1], dtype = 'int')\n",
    "X_test = np.loadtxt(location + cases[3] + labels[2], delimiter = ',')\n",
    "Y_test = np.loadtxt(location + cases[3] + labels[3], dtype = 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "6223dc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.eye(2)[Y_train]\n",
    "Y_test = np.eye(2)[Y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "7b7242e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MLP at 0x7fb378360b50>"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP(X_train, Y_train)\n",
    "model.Dense(64, 'sigmoid')\n",
    "model.OutputLayer()\n",
    "model.Momentum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "3968c7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.Training(200, 1, .04, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "354f1a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Test Accuracy ***: 0.978\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test dataset:\n",
    "pred, test_acc = model.Predict(X_test, Y_test)\n",
    "print(\"*** Test Accuracy ***: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "18ee3d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1]"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[int(x[1]) for x in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dae76ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
